<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if (IE 9)]><html class="no-js ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!-->
<html  lang="en-US">
	<head>
    	<title>Behavioral Prototype: Gesture recognition platform</title>
        
        <!-- Meta Tags -->
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="Personal webspace of Aniket Handa" />
        <!-- Mobile Internet Explorer ClearType Technology -->
        <!--[if IEMobile]>  <meta http-equiv="cleartype" content="on">  <![endif]-->
        
        <!-- Mobile Specifics -->
        <meta name="viewport" content="width=device-width">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="HandheldFriendly" content="true"/>
        <meta name="MobileOptimized" content="320"/>
        
        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	    <!--[if lt IE 9]>
	      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
	      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
	    <![endif]-->

        <!-- Bootstrap -->
        <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css">
        
        <!-- Custom CSS -->
        <link rel="stylesheet" href="/theme/css/main.css">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/theme/css/syntax.css">
        
        <!-- FancyBox -->
		<link href="/theme/css/fancybox/jquery.fancybox.css" rel="stylesheet">
		
		<!-- Font Icons -->
		<link href="/theme/css/fonts.css" rel="stylesheet">
		
		<!-- Google Font -->
		
		<link href='http://fonts.googleapis.com/css?family=Titillium+Web:600,600italic,200,200italic,300,300italic,400,400italic' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Roboto:400,100italic,100,300,300italic,400italic' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Roboto+Condensed:300italic,400italic,400,300' rel='stylesheet' type='text/css'>
		
		<!-- Fav Icon -->
		<link rel="icon" type="image/ico" href="favicon.ico">
		
		<link rel="apple-touch-icon" href="#">
		<link rel="apple-touch-icon" sizes="114x114" href="#">
		<link rel="apple-touch-icon" sizes="72x72" href="#">
		<link rel="apple-touch-icon" sizes="144x144" href="#">
		
		
		<!-- Analytics -->
		<script type="text/javascript">
		
		  var _gaq = _gaq || [];
		  _gaq.push(['_setAccount', 'UA-30930227-1']);
		  _gaq.push(['_trackPageview']);
		
		  (function() {
		    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
		    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		  })();
		
		</script>
		<!-- End Analytics -->

    </head>
    
    <body onload="">
    	
    	<!-- This section is for Splash Screen -->
		<section id="jSplash">
			<div id="circle"></div>
		</section>
		<!-- End of Splash Screen -->
		
		<!-- Header -->
		<header id="indexHeader">
		    <div class="sticky-nav">
		    	<a href="/">
			    	<div id="logo">
			        </div>
		        </a>

		        <!--

		        <nav id="menu">
				    <ul class="nav nav-tabs" id="pageTab">
						  <li class="active"><a href="#work">Portfolio</a></li>
						  <li><a href="#about">About</a></li>
						  <li style="text-decoration: line-through;"><a href="#">Playground</a></li>
					</ul>
		        </nav>

				-->

			</div>
		</header>
<!-- End Header -->

<!-- Header 
<header id="post">
    <div class="sticky-nav">
	    <a href="/">   
	        <div id="logo">
	        </div>
        </a>

        <div id="right">
        <ul id="rightList">
        	<li><a href="/"> <i class="fa fa-list-ul"></i></i></i></a></li>
        	
        		<li id = "rightNext"><a href="/course/playground/2014/02/10/UW-Q2-CSE512-A3.html"><i class="fa fa-chevron-left"></i></a></li>      
        	
        	
        		<li id = "rightPrev"><a href="/studio/playground/2014/02/05/UW-Q2-Studio-A3.html"><i class="fa fa-chevron-right"></i></a></li>
        	
        </ul>
        </div>       
    </div>
</header>
End Header -->

<div class="container">
	<div class="row">
		<div class="page">
			<h1>Behavioral Prototype: Gesture recognition platform</h1>
			<p class="meta" style="text-align: center">
				09 Feb 2014
			</p>
			<div class="post">
				<h2>Design Challenge</h2>

<p>The challenge for this assignment was to build and test a behavioral prototype for a gestural user interface for a TV system. They system had to allow for basic video function controls (play, pause, stop, fast forward, rewind, etc.) Our team chose to pursue a 3D gesture system. The goal of our prototype and evaluation was to explore the following design research and usability questions:</p>

<ul>
<li>How can a user effectively control video playback using hand gestures?</li>
<li>What are the most intuitive gestures for this application?</li>
<li>What level of accuracy is required in this gesture recognition technology?</li>
</ul>

<h2>Prototype</h2>

<p>We began by discussing how to set up our prototype to allow for quick iteration and modification. Two key elements to our prototype was </p>

<ol>
<li><p>figuring out how to manipulate the video content to simulate the controls carried out by a user’s 3D gestures outside of the user’s sight and </p></li>
<li><p>how to provide feedback to the user while carrying out a specific gestures. </p></li>
</ol>

<p>Initially we decided to use Google Chrome Cast to wirelessly control the television through a laptop. However, we soon realized that running Chrome Cast using a Mac had some glitches and delays that would negatively impact our test. We opted instead to use a mini display port to VGA adapter to connect the laptop directly into the television set. This allowed us to control the video in real time without any perceived delay from the user.
To simulate the feedback, we used a laser pointer aimed at the television that mimicked the user’s gesture. We also used this to indicate to the user when the gestures were outside of “the system’s” visible range. 
Next, we individually came up with a variety of gestures that we tested on ourselves and with each other. We noticed some common themes in the gestures we were developing and narrowed them down to three main sets, which included an open palm style, a fist style, and a thumb style of interaction. </p>

<h4>Gesture Sets – Palm, Fist, and Thumb</h4>

<p><img src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Studio/Q2/W4/Fist%20Instructions-01.png" alt="fist"></p>

<p><img src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Studio/Q2/W4/Palm%20Instructions-01.png" alt="palm"></p>

<p><img src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Studio/Q2/W4/Thumb%20Instructions-01.png" alt="thumb"></p>

<p>With these three varied sets of gestures we wanted to understand how users would perceive the required actions, how easy they would be to conduct, how intuitive each set was, how easy it would be to remember each set of gestures, and whether or not a user would prefer one set to the others. </p>

<p>Next, we decided on roles. We needed a Wizard/operator, a moderator, and a laser feedback controller. We also needed to recruit participants and find a location to conduct the test in the context of a home living room, where a television would likely be set up “in the wild”.</p>

<p><img src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Studio/Q2/W4/DSC_0039.JPG" alt="image"> </p>

<p>Images captured while setting up the behavior prototype. </p>

<p>In our final set up used for testing users in context, we had the moderator and user sitting on a sofa facing the television. The operator sat facing the user able to clearly see each of the user’s gestures, but out of the user’s direct attention. The laser feedback controller also stood out of the user’s attention facing the television in order to be able to recreate the user’s gestures while pointing the laser into the television. A camera set on a tripod was set up behind the user to simultaneously capture the user’s gestures and television response. </p>

<p>The user was given three sets of printed instructions describing each of the three gesture sets, which could be referenced at anytime during the test. A Kinect device was added to the television set up as a prop to represent the way in which the system would capture the user’s gesture input. </p>

<p><img src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Studio/Q2/W4/Behavioral%20Prototype%20Set%20Up.png" alt="setup"></p>

<p>For the evaluation itself, we put together a script describing what we were testing. We asked the user to review each set of instructions before performing the following predefined tasks while watching an episode of Sherlock:</p>

<ul>
<li>Start the video</li>
<li>Fast forward to wedding scene and play the video from there</li>
<li>Pause the video</li>
<li>Rewind to the scene where Sherlock &amp; Aunt are sharing tea</li>
<li>If you only wanted to see the last scene in the show, how would you get there?</li>
<li>Play again</li>
</ul>

<p>We also encouraged the user to use a think aloud protocol, so that we could capture his thoughts during the test.</p>

<p>After testing all three gesture sets, we surveyed the user to gather feedback on how easy it was to remember the gestures, gesture preferences, whether or not there were gestures that were particularly awkward or difficult to conduct, our feedback mechanism, and any abnormal or unexpected behaviors.  We ended with an open feedback session to allow the user to share any additional thoughts. </p>

<p>Here is the video of user testing:</p>

<div class="flex-video widescreen" style="margin: 0 auto;text-align:center;">
<iframe width="100%" height="100%" src="//www.youtube.com/embed/tMd0KVkJJ6c" frameborder="0"></iframe>
</div>

<h2>Analysis</h2>

<h4>What worked well</h4>

<p>Testing in a real living room was helpful for understanding the context in which a user would be using the product. This helped us realize quickly that gestures would most likely be conducted while users were sitting; so recommended gestures needed to take this into account. </p>

<p>Regarding gesture types, the user found the palm style and fist style gesture sets equality intuitive, but found the fist style gestures less easy to remember. The user had a preference for “the palm style…for sure because it’s the most simple...and the instructions were super simple.” </p>

<p>The user also noticed and appreciated the feedback of the green laser light because it was a “nice visual to know that action is registering on the screen.” He also stated that it was “helpful for tracking his motion.”</p>

<h4>What needed improvement</h4>

<p>Although, we felt that the thumb gestures could easily be recognized by they system because of the distinct directional cue provided by the thumb pointers created by the shape of the hand, it was not as well received by the user as the other gesture sets. The user found the rewind gesture for the thumb style set to be particularly awkward when using just the right hand, and introduced the idea of using the left hand for the same gesture. </p>

<p>The user felt that when performing fast forward and rewind using the fist style gestures, it was difficult to know how far to stretch his arm or how sensitive the system would be. </p>

<p>Although, the palm style set of gestures were preferred by the user, we noticed while analyzing the video that when the user repeated the fast forward gesture quickly, the motion could easy be misinterpreted for fast forward, rewind, fast forward, rewind, fast forward. Although the wizard knew the user’s intention and could operate the video correctly, it could be difficult for “the system” to distinguish the direction intent of the user. </p>

<p>We also realized through the testing that one of the main challenges, for all parties – user, Wizard, and laser operator alike, was understanding when the user’s actions were out of range. Our team discussed the possibility of providing some sort of calibration to find a users midpoint and mapping the video content timeline directly to the distance between the users hands when performing the most extreme fast forward and rewind gestures to make this experience more accurate.</p>

<p>Since our Wizard of Oz prototype did not account for other possible moving objects in the space, we felt it would be important to consider how the system would deal with “noise” from other moving objects during viewing. </p>

<h4>Conclusion</h4>

<p>Overall, we felt that our behavior prototype was successful in providing a “quick and dirty” way to test assumptions about the 3D gesture interactions we developed. We learned a great deal not only about the user’s gesture preferences, but also about the level of accuracy that the gesture recognition technology would need to account for.  </p>

			</div>
			<div id="disqus_thread"></div>
		    <script type="text/javascript">
		        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
		        var disqus_shortname = 'atneik'; // required: replace example with your forum shortname
		
		        /* * * DON'T EDIT BELOW THIS LINE * * */
		        (function() {
		            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		        })();
		    </script>
		    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
		</div>
	</div>

	
	<div class="row">
	<div class="col-xs-12 col-sm-6 nopadding">
	
				
		
			<a href="/RS.html">
			<div class="page-thumb">
				<div class="row">
					<div class="top-section">
						<heading>Intel® RealSense – Next Gen 3D Interfaces</heading>
						<tags>
						
							<tag>Intel Perceptual Computing SDK</tag> 
						
							<tag>3D gestures</tag> 
						
							<tag>Publication Industry</tag> 
						
						</tags>
					</div>
					<img class="img-responsive grayscale" src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Projects/stitch/ipad-stitch.jpeg">
					<div class="lower-section">
						<p> Now with upcoming Depth camera in mobile phone and tablets, our primary device will be better at sensing its surroundings. Can we create a more immersive content consumption medium using this technology? </p>
					</div>
				</div>
			</div>
			</a>
		
	
				
		
	
				
		
			<a href="/project/2014/04/27/UW-FBHack.html">
			<div class="page-thumb">
				<div class="row">
					<div class="top-section">
						<heading>Facebook Hackathon</heading>
						<tags>
						
							<tag>Front-End Development</tag> 
						
							<tag>Ideation</tag> 
						
							<tag>APIs</tag> 
						
						</tags>
					</div>
					<img class="img-responsive grayscale" src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Projects/showlist/showlist.jpeg">
					<div class="lower-section">
						<p> Won the 2nd position at the PNW regional Facebook Hackathon by designing and developing a platform to find and rate live concerts in 20 hours! </p>
					</div>
				</div>
			</div>
			</a>
		
	
				
		
	
				
		
			<a href="/project/2014/03/22/UW-DataVis.html">
			<div class="page-thumb">
				<div class="row">
					<div class="top-section">
						<heading>Data Visualization</heading>
						<tags>
						
							<tag>Tableau</tag> 
						
							<tag>D3.js</tag> 
						
							<tag>Front-End Engineering</tag> 
						
						</tags>
					</div>
					<img class="img-responsive grayscale" src="https://dl.dropboxusercontent.com/u/23289062/siteImages/DataViz/terror2.png">
					<div class="lower-section">
						<p> Projects during CSE 512 under Jeff Heer </p>
					</div>
				</div>
			</div>
			</a>
		
	
				
		
	
				
		
			<a href="/project/2014/01/12/JIIT-Skulpturous.html">
			<div class="page-thumb">
				<div class="row">
					<div class="top-section">
						<heading>Skulpturous</heading>
						<tags>
						
							<tag>Kinect</tag> 
						
							<tag>3D</tag> 
						
							<tag>Java</tag> 
						
							<tag>OpenNI</tag> 
						
						</tags>
					</div>
					<img class="img-responsive grayscale" src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Projects/skulp/skulpturous2.jpg">
					<div class="lower-section">
						<p> In this upcoming 3D revolution, can we bring 3D content creation to masses? </p>
					</div>
				</div>
			</div>
			</a>
		
	
				
		
	
	</div>
	<div class="col-xs-12 col-sm-6 nopadding">
	
				
		
	
				
		
		<a href="/project/2014/04/28/UW-NecX.html">
			<div class="page-thumb">
				<div class="row">
					<div class="top-section">
						<heading>NecX</heading>
						<tags>
						
							<tag>Arduino</tag> 
							<!--
							 / 
							-->
						
							<tag>Signal Processing</tag> 
							<!--
							 / 
							-->
						
							<tag>EMG sensor</tag> 
							<!--
							 / 
							-->
						
							<tag>Gesture detection</tag> 
							<!--
							 / 
							-->
						
							<tag>Machine Learning</tag> 
							<!--
							
							-->
						
						</tags>
					</div>
					<img class="img-responsive grayscale" src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Projects/NecX/neck.png">
					<div class="lower-section">
						<p> We took the idea of using Neck as an input method to a more sophisticated implementation during CSE477 Capstone course under Shwetak Patel </p>
					</div>
				</div>
			</div>
			</a>
		
	
				
		
	
				
		
		<a href="/project/2014/03/23/UW-Prototype.html">
			<div class="page-thumb">
				<div class="row">
					<div class="top-section">
						<heading>Rapid Prototyping</heading>
						<tags>
						
							<tag>Paper</tag> 
							<!--
							 / 
							-->
						
							<tag>Physical</tag> 
							<!--
							 / 
							-->
						
							<tag>Model</tag> 
							<!--
							 / 
							-->
						
							<tag>Mobile</tag> 
							<!--
							 / 
							-->
						
							<tag>Video</tag> 
							<!--
							 / 
							-->
						
							<tag>Website</tag> 
							<!--
							 / 
							-->
						
							<tag>Electronics</tag> 
							<!--
							 / 
							-->
						
							<tag>Arduino</tag> 
							<!--
							 / 
							-->
						
							<tag>Behavioral</tag> 
							<!--
							
							-->
						
						</tags>
					</div>
					<img class="img-responsive grayscale" src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Projects/ttt/nexus_framed-noShadow-low%20crop.png">
					<div class="lower-section">
						<p> Weekly prototyping chronicles </p>
					</div>
				</div>
			</div>
			</a>
		
	
				
		
	
				
		
		<a href="/project/2014/01/12/UW-RevealCare.html">
			<div class="page-thumb">
				<div class="row">
					<div class="top-section">
						<heading>Reveal Care</heading>
						<tags>
						
							<tag>User Centered Design</tag> 
							<!--
							 / 
							-->
						
							<tag>UI Prototype</tag> 
							<!--
							 / 
							-->
						
							<tag>AWS</tag> 
							<!--
							 / 
							-->
						
							<tag>Mobile</tag> 
							<!--
							
							-->
						
						</tags>
					</div>
					<img class="img-responsive grayscale" src="https://dl.dropboxusercontent.com/u/23289062/siteImages/Projects/revealCare/reveal%20care2.png">
					<div class="lower-section">
						<p> Can transparency in health care help us make wise medical decisions? </p>
					</div>
				</div>
			</div>
			</a>
		
	
				
		
	
				
		
		<a href="/project/2014/01/01/SideProjects.html">
			<div class="page-thumb">
				<div class="row">
					<div class="top-section">
						<heading>Recent side projects</heading>
						<tags>
						
							<tag>Arduino</tag> 
							<!--
							 / 
							-->
						
							<tag>Front-end development</tag> 
							<!--
							
							-->
						
						</tags>
					</div>
					<img class="img-responsive grayscale" src="https://raw.githubusercontent.com/HCDE498-598-Summer14/PresenDuino/master/Fritzing/PresenDuino.png">
					<div class="lower-section">
						<p>  </p>
					</div>
				</div>
			</div>
			</a>
		
	
	</div>
	</div>


	<!--
	<div class="row">
		
		<a href="/course/playground/2014/02/10/UW-Q2-CSE512-A3.html">
		<div class="col-md-6 page">
			<h2>Global Terrorism Visualization</h2>
			<strong>10 Feb 2014</strong>
			//
			
				The third assignment in CSE 512: Data Viz. was to find any compelling dataset and use interactions to aid exploration..
			
		</div>
		</a>
		
		
		<a href="/studio/playground/2014/02/05/UW-Q2-Studio-A3.html">
		<div class="col-md-6 page">
			<h2>Video Prototyping: Medical App</h2>
			<strong >05 Feb 2014</strong>
			//
			
				I try my hands on video prototyping and create a commercial-like video instead...
			
		</div>
		</a>
		
	</div>
	-->
</div>

        
        
		
		<!-- Js -->
		<!-- Modernizr -->
		<script src="/theme/js/modernizr.js"></script>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
		<script src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script> <!-- Bootstrap -->
		<link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
		
		<script src="/theme/js/waypoints.js"></script> <!-- WayPoints -->
		<script src="/theme/js/waypoints-sticky.js"></script> <!-- Waypoints for Header -->
		<script src="/theme/js/jquery.isotope.js"></script> <!-- Isotope Filter -->
		<script src="/theme/js/plugins.js"></script> <!-- Contains: jPreloader, jQuery Easing, jQuery ScrollTo, jQuery One Page Navi -->
		
		<script src="/theme/js/main.js"></script> <!-- Default JS -->
		<!-- End Js -->

    </body>
</html>
